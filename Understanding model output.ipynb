{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = 'smokin2mp4.pkl'\n",
    "with open(DATA_FILE_PATH, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data format: list of dicts for the analyzed frames of the following fields:\n",
    "{'frame_id', 'body_parts', 'all_peaks', 'subset', 'candiate'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCO_BODY_PARTS = ['nose', 'neck',\n",
    "                   'right_shoulder', ' right_elbow', 'right_wrist',\n",
    "                   'left_shoulder', 'left_elbow', 'left_wrist',\n",
    "                   'right_hip', 'right_knee', 'right_ankle',\n",
    "                   'left_hip', 'left_knee', 'left_ankle',\n",
    "                   'right_eye', 'left_eye', 'right_ear', 'left_ear', 'background'\n",
    "                   ]\n",
    "len(COCO_BODY_PARTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nose_x',\n",
       " 'nose_y',\n",
       " 'neck_x',\n",
       " 'neck_y',\n",
       " 'right_shoulder_x',\n",
       " 'right_shoulder_y',\n",
       " ' right_elbow_x',\n",
       " ' right_elbow_y',\n",
       " 'right_wrist_x',\n",
       " 'right_wrist_y',\n",
       " 'left_shoulder_x',\n",
       " 'left_shoulder_y',\n",
       " 'left_elbow_x',\n",
       " 'left_elbow_y',\n",
       " 'left_wrist_x',\n",
       " 'left_wrist_y',\n",
       " 'right_hip_x',\n",
       " 'right_hip_y',\n",
       " 'right_knee_x',\n",
       " 'right_knee_y',\n",
       " 'right_ankle_x',\n",
       " 'right_ankle_y',\n",
       " 'left_hip_x',\n",
       " 'left_hip_y',\n",
       " 'left_knee_x',\n",
       " 'left_knee_y',\n",
       " 'left_ankle_x',\n",
       " 'left_ankle_y',\n",
       " 'right_eye_x',\n",
       " 'right_eye_y',\n",
       " 'left_eye_x',\n",
       " 'left_eye_y',\n",
       " 'right_ear_x',\n",
       " 'right_ear_y',\n",
       " 'left_ear_x',\n",
       " 'left_ear_y']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_parts_xy = [[part + '_x', part + '_y'] for part in COCO_BODY_PARTS[0:-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['frame_id',\n",
       " 'bbox_x1',\n",
       " 'bbox_y1',\n",
       " 'bbox_x2',\n",
       " 'bbox_y2',\n",
       " 'nose_x',\n",
       " 'nose_y',\n",
       " 'neck_x',\n",
       " 'neck_y',\n",
       " 'right_shoulder_x',\n",
       " 'right_shoulder_y',\n",
       " 'right_elbow_x',\n",
       " 'right_elbow_y',\n",
       " 'right_wrist_x',\n",
       " 'right_wrist_y',\n",
       " 'left_shoulder_x',\n",
       " 'left_shoulder_y',\n",
       " 'left_elbow_x',\n",
       " 'left_elbow_y',\n",
       " 'left_wrist_x',\n",
       " 'left_wrist_y',\n",
       " 'right_hip_x',\n",
       " 'right_hip_y',\n",
       " 'right_knee_x',\n",
       " 'right_knee_y',\n",
       " 'right_ankle_x',\n",
       " 'right_ankle_y',\n",
       " 'left_hip_x',\n",
       " 'left_hip_y',\n",
       " 'left_knee_x',\n",
       " 'left_knee_y',\n",
       " 'left_ankle_x',\n",
       " 'left_ankle_y',\n",
       " 'right_eye_x',\n",
       " 'right_eye_y',\n",
       " 'left_eye_x',\n",
       " 'left_eye_y',\n",
       " 'right_ear_x',\n",
       " 'right_ear_y',\n",
       " 'left_ear_x',\n",
       " 'left_ear_y',\n",
       " 'config_score',\n",
       " 'no_joints',\n",
       " 'object_id']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_header = ['frame_id', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2'] + [l.strip(' ') for p in coco_parts_xy for l in p] + ['config_score', 'no_joints', 'object_id']\n",
    "print(len(array_header))\n",
    "array_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## body parts\n",
    "one example of coordiinates for every COCO body part. USELESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'nose': (230, 87), 'neck': (145, 100), 'right_shoulder': (228, 97), ' right_elbow': (127, 114), 'right_wrist': (130, 129), 'left_shoulder': (157, 100), 'left_elbow': (159, 121), 'left_wrist': (145, 133), 'right_hip': (229, 134), 'right_knee': (226, 156), 'right_ankle': (223, 174), 'left_hip': (154, 134), 'left_knee': (153, 160), 'left_ankle': (151, 184), 'right_eye': (229, 84), 'left_eye': (233, 84), 'right_ear': (138, 88), 'left_ear': (241, 85)},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_parts = np.array(data[25]['body_parts'])\n",
    "body_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_peaks\n",
    "a list of lists each corresponding to a COCO body part made of tuples of the following format of locs:\n",
    "(x, y, confidence, peak_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([(234, 87, 0.9083782434463501, 0), (181, 96, 0.5591726899147034, 1)]),\n",
       "       list([(169, 99, 0.765608012676239, 2), (241, 100, 0.9036915898323059, 3), (196, 118, 0.8418397307395935, 4)]),\n",
       "       list([(230, 97, 0.864372193813324, 5), (163, 100, 0.8208009600639343, 6), (210, 124, 0.8031818270683289, 7)]),\n",
       "       list([(227, 114, 0.7221102118492126, 8), (158, 120, 0.8832500576972961, 9), (215, 151, 0.8135343790054321, 10)]),\n",
       "       list([(178, 115, 0.6068695187568665, 11), (226, 129, 0.6727707982063293, 12), (232, 157, 0.5008898377418518, 13)]),\n",
       "       list([(177, 99, 0.5348705053329468, 14), (251, 103, 0.8400102257728577, 15), (182, 113, 0.7927486896514893, 16)]),\n",
       "       list([(252, 124, 0.8243544101715088, 17)]),\n",
       "       list([(244, 142, 0.7252730131149292, 18)]),\n",
       "       list([(167, 134, 0.8585205674171448, 19), (230, 134, 0.7712255120277405, 20), (204, 167, 0.680644690990448, 21)]),\n",
       "       list([(226, 155, 0.8229906558990479, 22), (167, 157, 0.8823679089546204, 23), (201, 197, 0.7733970284461975, 24)]),\n",
       "       list([(224, 175, 0.8872216939926147, 25), (165, 180, 0.9053114056587219, 26), (197, 223, 0.406983345746994, 27)]),\n",
       "       list([(177, 133, 0.7303912043571472, 28), (242, 137, 0.7648066282272339, 29), (189, 163, 0.6958182454109192, 30)]),\n",
       "       list([(175, 155, 0.7930884957313538, 31), (238, 161, 0.7691529393196106, 32), (187, 193, 0.7661322355270386, 33)]),\n",
       "       list([(173, 173, 0.8188678026199341, 34), (236, 184, 0.8541661500930786, 35), (184, 216, 0.4622024595737457, 36)]),\n",
       "       list([(233, 83, 0.8554295301437378, 37), (179, 93, 0.5754064917564392, 38)]),\n",
       "       list([(237, 84, 0.9530988931655884, 39), (184, 94, 0.269801527261734, 40)]),\n",
       "       list([(174, 90, 0.5507457852363586, 41), (207, 97, 0.7828583121299744, 42)]),\n",
       "       list([(244, 85, 0.8881293535232544, 43), (191, 96, 0.3839954733848572, 44)])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_peaks = np.array(data[0]['all_peaks'])\n",
    "all_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset\n",
    "an array in which each row corresponds to a different \"object\" with the row items indicating the peak_id's in all_peaks. the column index corresponds to the COCO body part index\n",
    "\n",
    "    # last number in each row is the total parts number of that person\n",
    "    # the second last number in each row is the score of the overall configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  3.        ,  5.        ,  8.        , 12.        ,\n",
       "        15.        , 17.        , 18.        , 20.        , 22.        ,\n",
       "        25.        , 29.        , 32.        , 35.        , 37.        ,\n",
       "        39.        , -1.        , 43.        , 28.89961997, 17.        ],\n",
       "       [-1.        ,  4.        ,  7.        , 10.        , 13.        ,\n",
       "        16.        , -1.        , -1.        , 21.        , 24.        ,\n",
       "        27.        , 30.        , 33.        , 36.        , -1.        ,\n",
       "        -1.        , 42.        , 44.        , 18.56997137, 13.        ],\n",
       "       [ 1.        ,  2.        ,  6.        ,  9.        , 11.        ,\n",
       "        14.        , -1.        , -1.        , 19.        , 23.        ,\n",
       "        26.        , 28.        , 31.        , 34.        , 38.        ,\n",
       "        40.        , 41.        , -1.        , 21.81312792, 15.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = np.array(data[0]['subset'])\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate\n",
    "array in which each row is one of the peaks, same as each tuple in all_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[234.        ,  87.        ,   0.90837824,   0.        ],\n",
       "       [181.        ,  96.        ,   0.55917269,   1.        ],\n",
       "       [169.        ,  99.        ,   0.76560801,   2.        ],\n",
       "       [241.        , 100.        ,   0.90369159,   3.        ],\n",
       "       [196.        , 118.        ,   0.84183973,   4.        ],\n",
       "       [230.        ,  97.        ,   0.86437219,   5.        ],\n",
       "       [163.        , 100.        ,   0.82080096,   6.        ],\n",
       "       [210.        , 124.        ,   0.80318183,   7.        ],\n",
       "       [227.        , 114.        ,   0.72211021,   8.        ],\n",
       "       [158.        , 120.        ,   0.88325006,   9.        ],\n",
       "       [215.        , 151.        ,   0.81353438,  10.        ],\n",
       "       [178.        , 115.        ,   0.60686952,  11.        ],\n",
       "       [226.        , 129.        ,   0.6727708 ,  12.        ],\n",
       "       [232.        , 157.        ,   0.50088984,  13.        ],\n",
       "       [177.        ,  99.        ,   0.53487051,  14.        ],\n",
       "       [251.        , 103.        ,   0.84001023,  15.        ],\n",
       "       [182.        , 113.        ,   0.79274869,  16.        ],\n",
       "       [252.        , 124.        ,   0.82435441,  17.        ],\n",
       "       [244.        , 142.        ,   0.72527301,  18.        ],\n",
       "       [167.        , 134.        ,   0.85852057,  19.        ],\n",
       "       [230.        , 134.        ,   0.77122551,  20.        ],\n",
       "       [204.        , 167.        ,   0.68064469,  21.        ],\n",
       "       [226.        , 155.        ,   0.82299066,  22.        ],\n",
       "       [167.        , 157.        ,   0.88236791,  23.        ],\n",
       "       [201.        , 197.        ,   0.77339703,  24.        ],\n",
       "       [224.        , 175.        ,   0.88722169,  25.        ],\n",
       "       [165.        , 180.        ,   0.90531141,  26.        ],\n",
       "       [197.        , 223.        ,   0.40698335,  27.        ],\n",
       "       [177.        , 133.        ,   0.7303912 ,  28.        ],\n",
       "       [242.        , 137.        ,   0.76480663,  29.        ],\n",
       "       [189.        , 163.        ,   0.69581825,  30.        ],\n",
       "       [175.        , 155.        ,   0.7930885 ,  31.        ],\n",
       "       [238.        , 161.        ,   0.76915294,  32.        ],\n",
       "       [187.        , 193.        ,   0.76613224,  33.        ],\n",
       "       [173.        , 173.        ,   0.8188678 ,  34.        ],\n",
       "       [236.        , 184.        ,   0.85416615,  35.        ],\n",
       "       [184.        , 216.        ,   0.46220246,  36.        ],\n",
       "       [233.        ,  83.        ,   0.85542953,  37.        ],\n",
       "       [179.        ,  93.        ,   0.57540649,  38.        ],\n",
       "       [237.        ,  84.        ,   0.95309889,  39.        ],\n",
       "       [184.        ,  94.        ,   0.26980153,  40.        ],\n",
       "       [174.        ,  90.        ,   0.55074579,  41.        ],\n",
       "       [207.        ,  97.        ,   0.78285831,  42.        ],\n",
       "       [244.        ,  85.        ,   0.88812935,  43.        ],\n",
       "       [191.        ,  96.        ,   0.38399547,  44.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate = np.array(data[0]['candidate'])\n",
    "candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data structure per person in video\n",
    "Try to create a 3D array of coordinates and time for each person\n",
    "\n",
    "1) parse pkl for coordinates for each body part per frame\n",
    "\n",
    "2) match same person between frames - based on the minimum change in head location between consecutive frames\n",
    "\n",
    "3) filter in time out bad pose estimates, based on:\n",
    "    a. smoothing filter\n",
    "    b. pose estimate confidence\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many persons there are in the video?\n",
    "Go over all frames and understand what is the average number of people.\n",
    "\n",
    "Choose representative middle frame from which to identify persons accross frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_persons_video(data, person_thresh=0): # TODO: need to figure out person configuration score threshold\n",
    "    no_persons = np.zeros(len(data))\n",
    "    for i, frame_data in enumerate(data):\n",
    "        subset = np.array(frame_data['subset'])\n",
    "        for person in subset:\n",
    "            if person[18] > person_thresh:\n",
    "                no_persons[i] += 1\n",
    "    return no_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 4., 4., 4., 3., 3., 3., 3., 3., 3., 4.,\n",
       "       3., 3., 3., 3., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "how_many_persons_video(data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frame level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peaks = np.array(data[0]['all_peaks'])\n",
    "subset = np.array(data[0]['subset'])\n",
    "candidate = np.array(data[0]['candidate'])\n",
    "person_thresh = 0 \n",
    "joint_thresh = 0\n",
    "\n",
    "def get_person_pose_frame(person, candidate, person_thresh=0, joint_thresh=0):\n",
    "    ''' extracts body positions for given person (item in the \"subset\" list)'''\n",
    "    confidence = person[18]     # check confidence level\n",
    "    if confidence < person_thresh:\n",
    "        return False\n",
    "    joints = person[0:18]\n",
    "    joints_locs = np.zeros((18, 2))\n",
    "    for i, joint in enumerate(joints):\n",
    "        j = int(joint)\n",
    "        if joint == -1:\n",
    "            joints_locs[i, :] = [-1, -1]\n",
    "        elif candidate[j, 2] < joint_thresh:\n",
    "            joints_locs[i, :] = [-1, -1]\n",
    "        else:\n",
    "            joints_locs[i, :] = candidate[j, 0:2]\n",
    "            \n",
    "    return joints_locs\n",
    "\n",
    "# check\n",
    "for person in subset:\n",
    "    joints_locs = get_person_pose_frame(person, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[181.,  96.],\n",
       "       [169.,  99.],\n",
       "       [163., 100.],\n",
       "       [158., 120.],\n",
       "       [178., 115.],\n",
       "       [177.,  99.],\n",
       "       [ inf,  inf],\n",
       "       [ inf,  inf],\n",
       "       [167., 134.],\n",
       "       [167., 157.],\n",
       "       [165., 180.],\n",
       "       [177., 133.],\n",
       "       [175., 155.],\n",
       "       [173., 173.],\n",
       "       [179.,  93.],\n",
       "       [184.,  94.],\n",
       "       [174.,  90.],\n",
       "       [ inf,  inf]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joints_locs[joints_locs <= 0] = np.inf\n",
    "joints_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching between persons in different frames\n",
    "So run previous function on all frames, see when getting head locations, and them match them based on minimal head loc change distanvce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-80db1dc3c36b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_no_people\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow_many_persons_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mjoints_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_joints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_no_people\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "# get max characters in video\n",
    "n_joints = 18\n",
    "head_loc_idx = 1\n",
    "max_no_people = max(how_many_persons_video(data))\n",
    "n_frames = len(data)\n",
    "joints_array = np.zeros((n_joints, 2, max_no_people, n_frames))\n",
    "\n",
    "for i, frame_data in enumerate(data):\n",
    "    subset = np.array(frame_data['subset'])\n",
    "\n",
    "    if i == 0:\n",
    "        for p, person in enumerate(subset):\n",
    "            joints_locs = get_person_pose_frame(person, candidate)\n",
    "            joints_array[:, :, p, i] = joints_locs\n",
    "            \n",
    "    else:\n",
    "        for p, person in enumerate(subset):\n",
    "            joints_locs = get_person_pose_frame(person, candidate)\n",
    "            head_loc = joints_loc[head_loc_idx, :]\n",
    "            dist_mat = pairwise_distances(head_loc, joints_array[head_loc_idx, :, :, i-1], \n",
    "                                          metric='euclidean', n_jobs=-1)\n",
    "            min_idx = np.argmin(dist_mat)\n",
    "            joints_array[:, :, p, i] = joints_locs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bounding box for head location per person to be used in SORT tracking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_pose_frame(person, candidate, person_thresh=0, joint_thresh=0):\n",
    "    ''' extracts body positions for given person (item in the \"subset\" list) '''\n",
    "    no_joints = person[19]\n",
    "    config_score = person[18]     # check configuration score\n",
    "    if config_score < person_thresh:\n",
    "        return False\n",
    "    joints = person[0:18]\n",
    "    joints_locs = np.zeros((18, 2))\n",
    "    for i, joint in enumerate(joints):\n",
    "        j = int(joint)\n",
    "        if joint == -1 or j >= len(candidate):\n",
    "            joints_locs[i, :] = [-1, -1]\n",
    "        elif candidate[j, 2] < joint_thresh:\n",
    "            joints_locs[i, :] = [-1, -1]\n",
    "        else:\n",
    "            joints_locs[i, :] = candidate[j, 0:2]\n",
    "            \n",
    "    return joints_locs, config_score, no_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_from_pose(joints_locs):\n",
    "    joints_locs_min = joints_locs.copy()\n",
    "    joints_locs_min[joints_locs <= 0] = np.inf\n",
    "    \n",
    "    minx = min(joints_locs_min[:, 0])\n",
    "    miny = min(joints_locs_min[:, 1])\n",
    "    maxx = max(joints_locs[:, 0])\n",
    "    maxy = max(joints_locs[:, 1])\n",
    "    return [minx, miny, maxx, maxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_center(bbox):\n",
    "    '''Returns the x, y coordinates of the center of a \n",
    "    bounding box of the following format [x1,y1,x2,y2] (opposite corners).\n",
    "    bbox = array of bounding boxes '''\n",
    "    if len(bbox.shape) == 1:\n",
    "        bbox = bbox.reshape(1, -1)\n",
    "    x = np.mean(bbox[:,[0, 2]], axis=1)\n",
    "    y = np.mean(bbox[:,[1, 3]], axis=1)\n",
    "    return np.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1 , 2, 3, 4])\n",
    "get_bbox_center(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_joints(data):\n",
    "    '''Function outputs an array of detected persons in each frame , \n",
    "    their joint body parts x, y locations and bounding box [x1,y1,x2,y2]'''\n",
    "    joints_array = []\n",
    "    for i, frame_data in enumerate(data):\n",
    "        # print(f'Processing frame {i}')\n",
    "        subset = np.array(frame_data['subset'])\n",
    "        for person in subset:\n",
    "            joints_locs, confidence, no_joints = get_person_pose_frame(person, candidate)\n",
    "            bboxes = get_bbox_from_pose(joints_locs)\n",
    "            joints_array.append([i] + list(bboxes) + list(joints_locs.flatten())+ [confidence, no_joints])\n",
    "    return np.array(joints_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "joints_array = get_all_joints(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         224.          83.         ...  28.89961997  17.\n",
      "    0.        ]\n",
      " [  0.         182.          96.         ...  18.56997137  13.\n",
      "    1.        ]\n",
      " [  0.         158.          90.         ...  21.81312792  15.\n",
      "    2.        ]\n",
      " ...\n",
      " [ 48.         158.          99.         ...  18.43457406  12.\n",
      "    3.        ]\n",
      " [ 49.         158.          87.         ...  15.64857325  12.\n",
      "    0.        ]\n",
      " [ 49.         163.          99.         ...  18.38607021  12.\n",
      "    3.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sort.sort import *\n",
    "\n",
    "#create instance of SORT\n",
    "mot_tracker = Sort() \n",
    "\n",
    "# get detections\n",
    "joints_array = get_all_joints(data)\n",
    "object_ids = []\n",
    "for i in range(len(data)):\n",
    "    detections = joints_array[joints_array[:, 0] == i, 1:5]\n",
    "    # update SORT\n",
    "    # trackers is a np array where each row contains a valid bounding box and track_id (last column)\n",
    "    trackers = mot_tracker.update(detections)\n",
    "    \n",
    "    # match detections and trackers based on bbox centers distance\n",
    "    det_centers = get_bbox_center(detections)\n",
    "    trk_centers = get_bbox_center(trackers[:,0:4])\n",
    "    indexes = pairwise_distances_argmin(det_centers, trk_centers, axis=0, metric='euclidean')\n",
    "    object_ids_frame = np.tile(-1, detections.shape[0])\n",
    "    object_ids_frame[indexes] = trackers[:, -1]\n",
    "    object_ids += list(object_ids_frame)\n",
    "    \n",
    "# concatenate object ids as new column to joints_array\n",
    "object_ids = np.array(object_ids)\n",
    "object_ids[object_ids >=0] = object_ids[object_ids >=0] - min(object_ids[object_ids >=0]) # reindex from 0\n",
    "joints_array = np.column_stack((joints_array, object_ids))\n",
    "print(joints_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(joints_array[:, -1].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create video with joints and bounding boxes\n",
    "## Canvas function from OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "filepath = os.path.join(data_dir, filename)\n",
    "with open(filepath, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(input_image, frame_data, no_objects, colors):\n",
    "    canvas = input_image.copy()\n",
    "    colors = []\n",
    "    for i, c in enumerate(colors):\n",
    "        cur_canvas = canvas.copy()\n",
    "        object_bbox = tuple(frame_data[frame_data[:, -1].astype(int) == i, 1:5].astype(int))\n",
    "        cv2.rectangle(cur_canvas, object_bbox, color=c,\n",
    "                                thickness=3)\n",
    "        canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "        \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_joints(input_image, all_peaks, subset, candidate, resize_fac=1):\n",
    "    canvas = input_image.copy()\n",
    "\n",
    "    for i in range(18):\n",
    "        for j in range(len(all_peaks[i])):\n",
    "            a = all_peaks[i][j][0] * resize_fac\n",
    "            b = all_peaks[i][j][1] * resize_fac\n",
    "            cv2.circle(canvas, (a, b), 2, util.colors[i], thickness=-1)\n",
    "\n",
    "    stickwidth = 4\n",
    "\n",
    "    for i in range(17):\n",
    "        for s in subset:\n",
    "            index = s[np.array(util.limbSeq[i]) - 1]\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            cur_canvas = canvas.copy()\n",
    "            y = candidate[index.astype(int), 0]\n",
    "            x = candidate[index.astype(int), 1]\n",
    "            m_x = np.mean(x)\n",
    "            m_y = np.mean(y)\n",
    "            length = ((x[0] - x[1]) ** 2 + (y[0] - y[1]) ** 2) ** 0.5\n",
    "            angle = math.degrees(math.atan2(x[0] - x[1], y[0] - y[1]))\n",
    "            polygon = cv2.ellipse2Poly((int(m_y * resize_fac), int(m_x * resize_fac)),\n",
    "                                       (int(length * resize_fac / 2), stickwidth), int(angle), 0, 360, 1)\n",
    "            cv2.fillConvexPoly(cur_canvas, polygon, util.colors[i])\n",
    "            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "        \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_generator(n):\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        colors.append((np.random.randint(255), np.random.randint(255), np.random.randint(255)))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start processing...')\n",
    "frame_rate_ratio = 3\n",
    "\n",
    "# Video input\n",
    "# video = videoname\n",
    "video_path = 'videos/'\n",
    "video_file = video_path + video\n",
    "\n",
    "# Output location\n",
    "output_path = 'videos/outputs/'\n",
    "output_format = '.mp4'\n",
    "video_output = output_path + video + str(start_datetime) + output_format\n",
    "\n",
    "# Video reader\n",
    "cam = cv2.VideoCapture(video_file)\n",
    "input_fps = cam.get(cv2.CAP_PROP_FPS)\n",
    "ret_val, orig_image = cam.read()\n",
    "video_length = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "ending_frame = video_length\n",
    "\n",
    "\n",
    "# Video writer\n",
    "output_fps = input_fps / frame_rate_ratio\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(video_output, fourcc, output_fps, (orig_image.shape[1], orig_image.shape[0]))\n",
    "\n",
    "# object (person) tracking\n",
    "no_objects = int(array[:, -1].max()) + 1\n",
    "colors = color_generator(no_objects)\n",
    "\n",
    "i = 0  # input video frame id\n",
    "j = 0  # analyzed frames id\n",
    "while(cam.isOpened()) and ret_val is True and i < ending_frame:\n",
    "    if i % frame_rate_ratio == 0:\n",
    "    \n",
    "        input_image = cv2.cvtColor(orig_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        tic = time.time()\n",
    "\n",
    "        # generate image with body parts\n",
    "        body_parts, all_peaks, subset, candidate = data[j, 1:5]\n",
    "        canvas = draw_joints(orig_image, all_peaks, subset, candidate)\n",
    "        \n",
    "        # draw bounding boxes based on data analyzed by joint_tracking.py\n",
    "        frame_data = array[array[:, 0].astype(int) == i, :]\n",
    "        canvas = draw_boxes(canvas, frame_data, no_objects, colors)\n",
    "        print('Processing frame: ', i)\n",
    "        toc = time.time()\n",
    "        print('processing time is %.5f' % (toc - tic))\n",
    "\n",
    "        out.write(canvas)\n",
    "        j += 1\n",
    "    ret_val, orig_image = cam.read()\n",
    "\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
